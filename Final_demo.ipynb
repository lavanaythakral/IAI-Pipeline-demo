{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final_demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanaythakral/IAI-Pipeline-demo/blob/main/Final_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcAxU-Tw4h4J"
      },
      "source": [
        "#Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG2Tlhjp4lbN"
      },
      "source": [
        "Code by: Lavanay Thakral\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "This Colab is a demo for my undergraduate thesis project. Since the code is a little difficult to navigate through, this introduction is meant to give a basic overview of how to work with it:\r\n",
        "\r\n",
        "*   The Web app Demo is hosted using this colab as the backend, and ngrok to redirect to a https url.\r\n",
        "*   SSLify is used to force an https url, because that is required by chrome to run the microphone access\r\n",
        "*   The flow goes as follows:\r\n",
        "    1.   Text input: This is sent as a POST request to the server\r\n",
        "    2.   Audio input (Microphone): This is converted to text using webkitSpeechRecognition and sent as a POST request to the server\r\n",
        "    3.   The input is converted into a sentence embedding using a BERT model (Bert as a service). This embedding is compared with the Questions cache, using cosine similarity as the metric. \r\n",
        "    4.   If the similarity is above a threshold, the Cached response and video is updated on the server.\r\n",
        "    5.   If that is not the case, we use TF-IDF vectorizer, and extract the most relevant context from the book, hoping that the answer to the input lies in that context; and use BERT for QNA to extract the answer from this context.\r\n",
        "    6.   If there is no response, we try to fetch the answer box result from a google search using the google API\r\n",
        "    7.   If the google search does not yield an answer box, we move to improvising, by passing the input as a prompt to GPT2 for generation. \r\n",
        "    8.   We extract our response from steps 5,6,7 and build audio with this text, using Google Text2Speech\r\n",
        "    9.   Further, using Wav2Lip model, speech audio, and a driving face video of Dr. Kalam, we animate to make a clip of Dr. Kalam speaking the response.\r\n",
        "    10.  Finally, this video is updated on the Web application. \r\n",
        "\r\n",
        "*  We believe the model does mimic Dr. Kalam's style but we don't claim or take responsibility of the accuracy of the model. We don't claim that the model is robust enough to say exactly what Dr. Kalam would have said.\r\n",
        "*  Such projects come with a lot of ethical issues around them. The authors have discussed these in detail in an article. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8-RlETXWdYA"
      },
      "source": [
        "#Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSe8gYUcROiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ab4bc9-1a56-4533-a9fd-1e90de6ef522"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV3YuVKlpd6v"
      },
      "source": [
        "!pip install flask\n",
        "!pip install flask_ngrok\n",
        "!pip install flask_sslify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNMd4A5vReqf"
      },
      "source": [
        "!git clone https://github.com/lavanaythakral/Kalam-AI-2.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVFezJQYp1o4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304ec34e-51cb-4c95-cf26-e7934a672677"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9PojOUdV9zt"
      },
      "source": [
        "!pip install google-cloud-texttospeech"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyg248DfWGTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a943f44-ab53-4e57-89d6-e4b5c55e02bf"
      },
      "source": [
        "cd Kalam-AI-2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Kalam-AI-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbEVd5yGWSnz"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f8Z2cRkWWCw"
      },
      "source": [
        "!apt-get install espeak\n",
        "!apt-get install portaudio19-dev python-pyaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvf8ciU2WaLu"
      },
      "source": [
        "!pip install PyAudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5rFmGbqWcDk"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import gpt_2_simple as gpt2\n",
        "from google.colab import drive\n",
        "import scipy\n",
        "import numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7cdj-RR0FCZ"
      },
      "source": [
        "Copying Fine tuned GPT2 model to workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw1Zt8GKXJhp"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1_topical_token')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqQQjsD90Xx9"
      },
      "source": [
        "Initializing credentials for Google Text2Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJpUGL9HXQR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870f9702-2aa0-4eef-e9e4-49fe6249726d"
      },
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/drive/MyDrive/Thesis project/My First Project-aa638edce601.json' \n",
        "!echo $GOOGLE_APPLICATION_CREDENTIALS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Thesis project/My First Project-aa638edce601.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txff4l0V13b8"
      },
      "source": [
        "Copying Wav2Lip models from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3zLUQEXXV3n"
      },
      "source": [
        "!cp -ri \"/content/drive/My Drive/Thesis project/Wav2Lip/Wav2Lip/wav2lip_gan.pth\" /content/Kalam-AI-2/Wav2Lip/checkpoints/\n",
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"/content/Kalam-AI-2/Wav2Lip/face_detection/detection/sfd/s3fd.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azT3k2yP03u3"
      },
      "source": [
        "Copying static files for the Web app from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbZPhFOz0b-f"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/Webapputils/static /content/Kalam-AI-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvoLjBS72CUy"
      },
      "source": [
        "Initializing BERT for Q/A, BERT For NSP and Fine-tuned GPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89TkKcsiXb05"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, BertForNextSentencePrediction, BertTokenizer\n",
        "import gpt_2_simple as gpt2\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1_topical_token')\n",
        "\n",
        "print(\"GPT2 loaded\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-large-uncased-whole-word-masking-squad2\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-large-uncased-whole-word-masking-squad2\")\n",
        "\n",
        "print(\"BERT For Q/A downloaded\")\n",
        "\n",
        "model_nsp = BertForNextSentencePrediction.from_pretrained('bert-base-cased')\n",
        "tokenizer_nsp = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "print(\"BERT NSP downloaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyX6TcMo2Otd"
      },
      "source": [
        "Loading Cache responses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO2oLspQbCZU"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Thesis project/Thesis project/datasets/database_new1.csv')\n",
        "index_database = {}\n",
        "data_questions = list(df['Question'])\n",
        "for idx,row in df.iterrows():\n",
        "  index_database[row['Question'].lower()] = [row['Index'],row['Answer'],row['Source']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jnw4ye_X3YV"
      },
      "source": [
        "#Script Functions.\r\n",
        "(These may be converted to python scripts for ease. Here, I have kept all the functions in the colab itself)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5cca6rhX1oz"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import re\n",
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "websites = \"[.](com|net|org|io|gov)\"\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKDDCURU2d1L"
      },
      "source": [
        "Grammar correction scripts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9PszvzxX4Oc"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"Simple grammar checker\n",
        "\n",
        "This grammar checker will fix grammar mistakes using Ginger.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "from urllib.error import URLError\n",
        "import json\n",
        "\n",
        "\n",
        "class ColoredText:\n",
        "    \"\"\"Colored text class\"\"\"\n",
        "    colors = ['black', 'red', 'green', 'orange', 'blue', 'magenta', 'cyan', 'white']\n",
        "    color_dict = {}\n",
        "    for i, c in enumerate(colors):\n",
        "        color_dict[c] = (i + 30, i + 40)\n",
        "\n",
        "    @classmethod\n",
        "    def colorize(cls, text, color=None, bgcolor=None):\n",
        "        \"\"\"Colorize text\n",
        "        @param cls Class\n",
        "        @param text Text\n",
        "        @param color Text color\n",
        "        @param bgcolor Background color\n",
        "        \"\"\"\n",
        "        c = None\n",
        "        bg = None\n",
        "        gap = 0\n",
        "        if color is not None:\n",
        "            try:\n",
        "                c = cls.color_dict[color][0]\n",
        "            except KeyError:\n",
        "                print(\"Invalid text color:\", color)\n",
        "                return(text, gap)\n",
        "\n",
        "        if bgcolor is not None:\n",
        "            try:\n",
        "                bg = cls.color_dict[bgcolor][1]\n",
        "            except KeyError:\n",
        "                print(\"Invalid background color:\", bgcolor)\n",
        "                return(text, gap)\n",
        "\n",
        "        s_open, s_close = '', ''\n",
        "        if c is not None:\n",
        "            s_open = '\\033[%dm' % c\n",
        "            gap = len(s_open)\n",
        "        if bg is not None:\n",
        "            s_open += '\\033[%dm' % bg\n",
        "            gap = len(s_open)\n",
        "        if not c is None or bg is None:\n",
        "            s_close = '\\033[0m'\n",
        "            gap += len(s_close)\n",
        "        return('%s%s%s' % (s_open, text, s_close), gap)\n",
        "\n",
        "\n",
        "def get_ginger_url(text):\n",
        "    \"\"\"Get URL for checking grammar using Ginger.\n",
        "    @param text English text\n",
        "    @return URL\n",
        "    \"\"\"\n",
        "    API_KEY = \"6ae0c3a0-afdc-4532-a810-82ded0054236\"\n",
        "\n",
        "    scheme = \"http\"\n",
        "    netloc = \"services.gingersoftware.com\"\n",
        "    path = \"/Ginger/correct/json/GingerTheText\"\n",
        "    params = \"\"\n",
        "    query = urllib.parse.urlencode([\n",
        "        (\"lang\", \"US\"),\n",
        "        (\"clientVersion\", \"2.0\"),\n",
        "        (\"apiKey\", API_KEY),\n",
        "        (\"text\", text)])\n",
        "    fragment = \"\"\n",
        "\n",
        "    return(urllib.parse.urlunparse((scheme, netloc, path, params, query, fragment)))\n",
        "\n",
        "\n",
        "def get_ginger_result(text):\n",
        "    \"\"\"Get a result of checking grammar.\n",
        "    @param text English text\n",
        "    @return result of grammar check by Ginger\n",
        "    \"\"\"\n",
        "    url = get_ginger_url(text)\n",
        "\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "    except HTTPError as e:\n",
        "            print(\"HTTP Error:\", e.code)\n",
        "            quit()\n",
        "    except URLError as e:\n",
        "            print(\"URL Error:\", e.reason)\n",
        "            quit()\n",
        "\n",
        "    try:\n",
        "        result = json.loads(response.read().decode('utf-8'))\n",
        "    except ValueError:\n",
        "        print(\"Value Error: Invalid server response.\")\n",
        "        quit()\n",
        "\n",
        "    return(result)\n",
        "\n",
        "\n",
        "def fix(original_text):\n",
        "    \"\"\"main function\"\"\"\n",
        "    # original_text = \" \".join(sys.argv[1:])\n",
        "    # if len(original_text) > 600:\n",
        "    #     print(\"You can't check more than 600 characters at a time.\")\n",
        "    #     quit()\n",
        "    fixed_text = original_text\n",
        "    results = get_ginger_result(original_text)\n",
        "    print(results)\n",
        "    # Correct grammar\n",
        "    if(not results[\"LightGingerTheTextResult\"]):\n",
        "        print(\"Good English :)\")\n",
        "        quit()\n",
        "\n",
        "    # Incorrect grammar\n",
        "    color_gap, fixed_gap = 0, 0\n",
        "    for result in results[\"LightGingerTheTextResult\"]:\n",
        "        if(result[\"Suggestions\"]):\n",
        "            from_index = result[\"From\"] + color_gap\n",
        "            to_index = result[\"To\"] + 1 + color_gap\n",
        "            suggest = result[\"Suggestions\"][0][\"Text\"]\n",
        "\n",
        "            # Colorize text\n",
        "            colored_incorrect = ColoredText.colorize(original_text[from_index:to_index], 'red')[0]\n",
        "            colored_suggest, gap = ColoredText.colorize(suggest)\n",
        "\n",
        "            original_text = original_text[:from_index] + colored_incorrect + original_text[to_index:]\n",
        "            fixed_text = fixed_text[:from_index-fixed_gap] + colored_suggest + fixed_text[to_index-fixed_gap:]\n",
        "            # fixed_text = fixed_text[:from_index-fixed_gap] + suggest + fixed_text[to_index-fixed_gap:]\n",
        "\n",
        "            color_gap += gap\n",
        "            fixed_gap += to_index-from_index-len(suggest)\n",
        "\n",
        "    print(\"from: \" + original_text)\n",
        "    print(\"to:   \" + fixed_text)\n",
        "    return fixed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr88xBPUX_xY"
      },
      "source": [
        "ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYRXrr0t2jpB"
      },
      "source": [
        "Fetching Result from Google API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yrjr-wgYHfE"
      },
      "source": [
        "#fetch_google\n",
        "import yake\n",
        "import requests\n",
        "\n",
        "API_KEY = '4e1eb040-18e7-11eb-a969-4d03007837e9'\n",
        "\n",
        "def keys(query):\n",
        "  kw_extractor = yake.KeywordExtractor()\n",
        "  keywords = kw_extractor.extract_keywords(query)\n",
        "  txt = \"A P J Abdul Kalam\"\n",
        "  for kw in keywords:\n",
        "    # print(kw[1])\n",
        "    txt += \" \"\n",
        "    txt += kw[1]\n",
        "  return txt,keywords\n",
        "\n",
        "def get_ans(question):\n",
        "  headers = { 'apikey': API_KEY }\n",
        "  params = (\n",
        "    (\"q\",question),\n",
        "    (\"device\",\"desktop\"),\n",
        "    (\"gl\",\"IN\"),\n",
        "    (\"hl\",\"en\"),\n",
        "    (\"location\",\"Navi Mumbai,Maharashtra,India\"),\n",
        "    (\"num\",\"50\"),\n",
        "  )\n",
        "\n",
        "  response = requests.get('https://app.zenserp.com/api/v2/search', headers=headers, params=params)\n",
        "  data = response.json()\n",
        "  if 'answer_box' in data.keys():\n",
        "    res = data['answer_box']['answer']\n",
        "  else:\n",
        "    res = \"No clue\"\n",
        "  return res\n",
        "\n",
        "def master(query):\n",
        "  txt,keywords = keys(query)\n",
        "  # print(txt)\n",
        "  res = get_ans(txt)\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if42tGeM2sCp"
      },
      "source": [
        "BERT for QNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll_uODaaYKY0",
        "outputId": "5e66671b-7342-4992-9716-7bc2e3a516b0"
      },
      "source": [
        "#qnautils\n",
        "import torch\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords as sw, wordnet as wn\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# from fetch_google import *\n",
        "\n",
        "def get_data():\n",
        "  df = pd.read_csv('WOF_split_into_sentences.csv')\n",
        "  sentences = list(df['Sentences'])\n",
        "  print(\"Number of sentences in the dataframe : \",len(sentences))\n",
        "  return df\n",
        "\n",
        "def get_context_from_data(query,df):\n",
        "  sentences = list(df['Sentences'])\n",
        "  vectorizer = TfidfVectorizer(stop_words='english')\n",
        "  X = vectorizer.fit_transform(sentences)\n",
        "  X = normalize(X)\n",
        "  print(\"Data vectorization completed\")\n",
        "  Question = vectorizer.transform([query])\n",
        "  Question = normalize(Question)\n",
        "  cosineSimilarities = cosine_similarity(Question, X).flatten()\n",
        "  idx = cosineSimilarities.argsort()[::-1][:20]\n",
        "  temp = \"\"\n",
        "  # print(query)\n",
        "  for i in idx:                                                                             \n",
        "    if(cosineSimilarities[i] != 0):\n",
        "      temp = temp + sentences[i]\n",
        "  print(\"Context has been extracted\")\n",
        "  return temp\n",
        "\n",
        "def qna(questions,df,model,tokenizer):\n",
        "  answers = []\n",
        "  for question in questions:\n",
        "    print(\"Looking for answer for question : \", question)\n",
        "    text = get_context_from_data(question,df)\n",
        "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer_start_scores, answer_end_scores = model(**inputs).values()\n",
        "    print(\"_________\",answer_start_scores)\n",
        "    answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(\"Answer found\\n\")\n",
        "    answers.append(answer)\n",
        "  return answers\n",
        "\n",
        "def Fetching_answers(test_questions,df,model,tokenizer):\n",
        "  answers = qna(test_questions,df,model,tokenizer)\n",
        "  for idx,ans in enumerate(answers):\n",
        "    if ans == '[CLS]' or ans == '':\n",
        "      res = master(test_questions[idx])\n",
        "      if res == 'No clue':\n",
        "        answers[idx] = 'PASS' \n",
        "      else:\n",
        "        answers[idx] = res\n",
        "  return answers\n",
        "\n",
        "\n",
        "def ner(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  entities = []\n",
        "  for ent in doc.ents:\n",
        "    entities.append([ent.text,ent.label_])\n",
        "  return entities\n",
        "\n",
        "def entities(sentences,answers):\n",
        "  for idx,sen in enumerate(sentences):\n",
        "    if(answers[idx] == 'PASS' and len(ner(sen)) == 0):\n",
        "      print(sen)\n",
        "      answers[idx] = 'GPT2'\n",
        "  return answers\n",
        "\n",
        "def phase_one_end(questions,df,model,tokenizer):\n",
        "  ans = Fetching_answers(questions,df,model,tokenizer)\n",
        "  final = entities(questions,ans)\n",
        "  return final\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBSmekzO20gJ"
      },
      "source": [
        "GPT2 Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oPDiR1EYNft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a57a45-c8ea-41ef-c63e-486ead31ab68"
      },
      "source": [
        "#improvutils\n",
        "import gpt_2_simple as gpt2\n",
        "from torch.nn.functional import softmax\n",
        "from transformers import BertForNextSentencePrediction,BertTokenizer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "import re\n",
        "\n",
        "def generate_candidates(input,sess):\n",
        "\tprint(\"GPT2 generating for :\",input)\n",
        "\tgenerated_text = gpt2.generate(sess,\n",
        "\t\tlength=100,\n",
        "\t\trun_name='run1_topical_token',\n",
        "\t\treturn_as_list=True,\n",
        "\t\ttemperature=0.7,\n",
        "\t\tprefix=input,\n",
        "\t\tnsamples=15,\n",
        "\t\ttruncate = '.',\n",
        "\t\tbatch_size=5,\n",
        "\t\ttop_k = 5,\n",
        "\t\tinclude_prefix = False)\n",
        "\n",
        "\tdef clean(input_st, sub):\n",
        "\t\treturn input_st.replace(sub, '').lstrip()\n",
        "\n",
        "\tcleaned = []\n",
        "\tfor text in generated_text:\n",
        "\t\tcleaned.append(re.sub(r\"^\\W+\", \"\",clean(text,'<|endoftext|>')))\n",
        "\n",
        "\treturn cleaned\n",
        "\n",
        "\n",
        "def top_result(seq_A,seq_B,model,tokenizer):\n",
        "\tresponse = seq_B[0]\n",
        "\tmax_prob = -1\n",
        "\tfor seq in seq_B:\n",
        "\t\tencoded = tokenizer.encode_plus(seq_A, text_pair=seq, return_tensors='pt')\n",
        "\t\tseq_relationship_logits = model(**encoded)[0]\n",
        "\t\tprobs = softmax(seq_relationship_logits, dim=1)\n",
        "\t\tif probs[0][0] > max_prob:\n",
        "\t\t\tmax_prob = probs[0][0]\n",
        "\t\t\tresponse = seq\n",
        "\t\tif max_prob >= 0.97:\n",
        "\t\t\treturn response\n",
        "\telse:\n",
        "\t\treturn -1\n",
        "\n",
        "\n",
        "def master_GPT2(inp,model,tokenizer,sess):\n",
        "  generation_cleaned = generate_candidates(inp,sess)\n",
        "  candidates = []\n",
        "  for gen in generation_cleaned:\n",
        "    if len(ner(gen)) == 0:\n",
        "      candidates.append(gen)\n",
        "  print(candidates)\n",
        "  \n",
        "  res = top_result(inp,candidates,model,tokenizer)\n",
        "  polarity = sid.polarity_scores(res)['compound']\n",
        "  return res,polarity\n",
        "\n",
        "def regeneration(inp,words,model,tokenizer,sess):\n",
        "  flg = 1\n",
        "  resp = \"\"\n",
        "  pol = \"\"\n",
        "  \n",
        "  while(flg == 1):\n",
        "    flg = 0\n",
        "    resp,pol = master_GPT2(inp,model,tokenizer,sess)\n",
        "    txt,keywords = keys(resp)\n",
        "    print(resp,pol)\n",
        "    if pol > 0.0:\n",
        "      polarity = 'positive'\n",
        "    elif pol < 0.0:\n",
        "      polarity = 'negative'\n",
        "    else:\n",
        "      polarity = 'neutral'\n",
        "    for x in keywords:\n",
        "      if x in words.keys() and polarity != words[x]:\n",
        "        flg = 1\n",
        "  return resp\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCiSmrIt27KF"
      },
      "source": [
        "Google Text2Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28MnBM9UYQDk"
      },
      "source": [
        "def synthesize_text(text):\n",
        "    \"\"\"Synthesizes speech from the input string of text.\"\"\"\n",
        "    from google.cloud import texttospeech\n",
        "\n",
        "    client = texttospeech.TextToSpeechClient()\n",
        "\n",
        "    input_text = texttospeech.SynthesisInput(text=text)\n",
        "\n",
        "    # Note: the voice can also be specified by name.\n",
        "    # Names of voices can be retrieved with client.list_voices().\n",
        "    voice = texttospeech.VoiceSelectionParams(\n",
        "        language_code=\"en-US\",\n",
        "        name=\"en-US-Wavenet-J\",\n",
        "        ssml_gender=texttospeech.SsmlVoiceGender.MALE,\n",
        "    )\n",
        "\n",
        "    audio_config = texttospeech.AudioConfig(\n",
        "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
        "    )\n",
        "\n",
        "    response = client.synthesize_speech(\n",
        "        request={\"input\": input_text, \"voice\": voice, \"audio_config\": audio_config}\n",
        "    )\n",
        "\n",
        "    # The response's audio_content is binary.\n",
        "    with open('t2s.mp3', \"wb\") as out:\n",
        "        out.write(response.audio_content)\n",
        "        print('Audio content written to file \"output.mp3\"')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VlM8zyEYWnk"
      },
      "source": [
        "#Bert as a service\r\n",
        "Used to convert questions into embeddings, to compare with the closest match in the cache"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h4gJf4UYSB1"
      },
      "source": [
        "!pip install bert-serving-client\n",
        "!pip install -U bert-serving-server[http]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGPpA6PcYZ_T"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip\n",
        "!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I_E4Qbbq5bD"
      },
      "source": [
        "data_questions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkwY2ATWYcAB"
      },
      "source": [
        "from bert_serving.client import BertClient\n",
        "bc = BertClient(check_length = False)\n",
        "question_bert_embeddings = bc.encode(data_questions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhBJ1Mx_YeX_"
      },
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_similar_question(sentence,data_questions,question_embeddings):\n",
        "  sentence_embedding = bc.encode([sentence])\n",
        "  maxm = -1\n",
        "  id = 0\n",
        "  for j,question in enumerate(question_embeddings):\n",
        "    dist = dot(question, sentence_embedding[0])/(norm(question)*norm(sentence_embedding[0]))\n",
        "    if(dist > maxm):\n",
        "      maxm = dist\n",
        "      id = j\n",
        "  return data_questions[id],maxm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv81eqTaYjOE"
      },
      "source": [
        "#Server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkgJvQgg4hM2"
      },
      "source": [
        "from flask import Flask, render_template, request, jsonify\r\n",
        "from flask_ngrok import run_with_ngrok\r\n",
        "from flask_sslify import SSLify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFqW1pnzsXr8"
      },
      "source": [
        "import time\r\n",
        "times = []\r\n",
        "\r\n",
        "app = Flask(__name__)\r\n",
        "sslify = SSLify(app)\r\n",
        "app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0\r\n",
        "run_with_ngrok(app)  \r\n",
        "\r\n",
        "@app.route('/') # default route\r\n",
        "def new():\r\n",
        "  return render_template('index_final.html') \r\n",
        "\r\n",
        "@app.route('/process', methods = ['POST'])\r\n",
        "def process():\r\n",
        "  curr = time.time()\r\n",
        "  threshold_score = 0.5\r\n",
        "  \r\n",
        "  name = request.get_data().decode('utf-8')\r\n",
        "  print(\"The text is\", name)\r\n",
        "  \r\n",
        "  result = \"\"\r\n",
        "  video_filename = \"\"\r\n",
        "  source = \"\"\r\n",
        "  cached = True\r\n",
        "  \r\n",
        "  if name.lower() in index_database:\r\n",
        "    print(\"Response found in the cache\")\r\n",
        "    video_filename = str(index_database[name.lower()][0]) + \".mp4\"\r\n",
        "    result = index_database[name.lower()][1]\r\n",
        "    source = index_database[name.lower()][2]\r\n",
        "    video_filename = \"/static/Video_cache2/\" + video_filename\r\n",
        "\r\n",
        "  else:\r\n",
        "    similarity_response = find_similar_question(name,data_questions,question_bert_embeddings)\r\n",
        "    sim_qs = similarity_response[0]\r\n",
        "    sim_score = similarity_response[1]\r\n",
        "    print(sim_score)\r\n",
        "    \r\n",
        "    if (sim_score > threshold_score):\r\n",
        "      print(\"Similar response found in the cache\")\r\n",
        "      video_filename = str(index_database[sim_qs.lower()][0]) + \".mp4\"\r\n",
        "      result = index_database[sim_qs.lower()][1]\r\n",
        "      source = index_database[sim_qs.lower()][2]\r\n",
        "      video_filename = \"/static/Video_cache2/\" + video_filename\r\n",
        "\r\n",
        "    else:\r\n",
        "      cached = False\r\n",
        "      answers = phase_one_end([name],data,model,tokenizer)\r\n",
        "      if (answers[0] != 'GPT2' and answers[0] != 'PASS' and answers[0] != '[CLS]'):\r\n",
        "        print(\"Response extracted\")\r\n",
        "        result = answers[0]\r\n",
        "        source = \"Fact\"\r\n",
        "      else:\r\n",
        "        print(\"Response generated\")\r\n",
        "        result = regeneration(query,words,model_nsp,tokenizer_nsp,sess)\r\n",
        "        source = \"GPT2\"\r\n",
        "\r\n",
        "  sen = split_into_sentences(result)\r\n",
        "  \r\n",
        "  if (len(sen) == 0):\r\n",
        "    result = result + \".\"\r\n",
        "  else:\r\n",
        "    result = sen[0]\r\n",
        "  \r\n",
        "  result = fix(result)\r\n",
        "  result = ansi_escape.sub('', result)\r\n",
        "  \r\n",
        "  print(result)\r\n",
        "  print(source)\r\n",
        "\r\n",
        "  if (cached == False):\r\n",
        "    !rm -f /content/Kalam-AI-2/static/result_voice.mp4\r\n",
        "    synthesize_text(result)\r\n",
        "    !cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"/static/Final-speaking_111.mp4\" --audio \"t2s.mp3\" \r\n",
        "    !cp -ru \"/content/Kalam-AI-2/Wav2Lip/results/result_voice.mp4\" \"/content/Kalam-AI-2/static\"\r\n",
        "    video_filename = \"/static/result_voice.mp4\"\r\n",
        "  \r\n",
        "  times.append([name,time.time() - curr])\r\n",
        "  return jsonify(result = result, vid = video_filename, source = source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx3BbPA34c0L"
      },
      "source": [
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_C58tiXTbKg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}